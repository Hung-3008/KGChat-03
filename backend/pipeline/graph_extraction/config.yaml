graph_extraction:
  provider: "gemini"  # "ollama" or "gemini"
  models:
    #- "llama3.1:8b"
    - "gemini-2.0-flash"
  datasets:
    - "500 samples pmc"
    #- "NCBItestset_corpus"
    #- "bc5dr"
  prompt_settings:
    base_path: "test/nodes_extraction/prompts/set_08"
    mode: "standard"  # "interactive" or "standard"
    k: 3 # number of refinement loops for interactive mode
    node_system_prompt: "node_system_prompt.txt"
    node_user_prompt: "zeroshot_prompt.txt" # or "fewshot_prompt.txt" or "cot_prompt.txt"
    feedback_prompt: "feedback_prompt.txt"

    edge_system_prompt: "edge_system_prompt.txt"
    edge_user_prompt: "edge_cot_prompt.txt" # or "edge_fewshot_prompt.txt" or "edge_cot_prompt.txt"

  max_samples: 3 # set to a number or null to process all samples
  time_between_requests: 3 # seconds to wait between API calls (e.g., for rate limiting)
  fuzzy_threshold: 0.7
